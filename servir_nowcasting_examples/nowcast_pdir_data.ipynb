{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nowcasting PDIR data\n",
    "\n",
    "This notebook provide instructions on how to nowcast using the data that was previously downloaded. This notebook will cover how to \n",
    "\n",
    "1. Use the dataloader from the h5 files to produce windowed inputs\n",
    "2. Initialize config files for each nowcasting model\n",
    "3. Nowcast the generated inputs and save the outputs in a format ready to be used for downstream tasks\n",
    "\n",
    "Author: Akshay Aravamudan (aaravamudan2014@my.fit.edu)\n",
    "\n",
    "Last Edited: Jan 21, 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pysteps configuration file found at: /volume/NFS/aa3328/miniconda3/envs/tito_env/lib/python3.12/site-packages/pysteps/pystepsrc\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/volume/NFS/aa3328/miniconda3/envs/tito_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from servir.utils.evaluation import generate_outputs\n",
    "from servir.core.data_provider import IMERGDataModule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# event id for which the data was downloaded\n",
    "event_id = 'PDIR_data'\n",
    "\n",
    "# location of the h5 file that was generated after downloading the data\n",
    "h5_dataset_location = '../data/events/'+str(event_id)+'.h5'\n",
    "\n",
    "# as of now, we do not have IR data, so we set it None\n",
    "ir_h5_dataset_location = None\n",
    "\n",
    "# this string is used to determine the kind of dataloader we need to use\n",
    "# for processing individual events, we reccommend the user to keep this fixed\n",
    "dataset_type = 'wa'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original shape (9, 360, 516)\n",
      "Precipitation Dataset input shape:  (0, 8, 1, 360, 516)\n"
     ]
    }
   ],
   "source": [
    "data_provider =  IMERGDataModule(\n",
    "        forecast_steps = 12,\n",
    "        history_steps = 8,\n",
    "        imerg_filename = h5_dataset_location,\n",
    "        ir_filename = ir_h5_dataset_location,\n",
    "        batch_size = 32,\n",
    "        image_shape = (360, 516),\n",
    "        normalize_data=False,\n",
    "        dataset = dataset_type,\n",
    "        production_mode = True)\n",
    "\n",
    "# for now we are treating the test dataloader as the main one since we are only interested in predicting for individual events\n",
    "data_loader = data_provider.test_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize config files for individual models and predict\n",
    "\n",
    "For producing teh nowcasts using each of the models, the following parameters need to be initialized\n",
    "\n",
    "1. model_type: name of the model ('convlstm', 'linda', 'steps', 'lagrangian', 'naive')\n",
    "2. model_config_location: location where the python file with model specific parameters can be found. These have already been populated in the `configs/wa_imerg/` directory.\n",
    "3. model_save_location: if the model requires a saved object (for instance neural network weights), this parameter points to that location.\n",
    "4. use_gpu: flag for whether the model should use a GPU. note that this is only applicable for neural network models and the node which runs this notebook must contain a GPU.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConvLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading config from configs/wa_imerg/ConvLSTM.py ...\n",
      "loading config from configs/wa_imerg/ConvLSTM.py ...\n"
     ]
    }
   ],
   "source": [
    "from servir.utils.m_h5py2tif import h5py2tif\n",
    "from servir.core.model_picker import ModelPicker\n",
    "\n",
    "model_type = 'convlstm'\n",
    "model_config_location = 'configs/wa_imerg/ConvLSTM.py'\n",
    "model_save_location = 'temp/imerg_only_mse_params.pth'\n",
    "model_output_location = 'results/'\n",
    "use_gpu = False\n",
    "produce_ensemble_outputs = False\n",
    "convlstm_output = generate_outputs(data_loader, model_type, model_config_location, model_save_location, use_gpu, produce_ensemble_outputs=produce_ensemble_outputs, event_name = event_id, model_output_location=model_output_location)\n",
    "\n",
    "model_picker = ModelPicker(model_type=model_type, \n",
    "                            model_config_location=model_config_location, \n",
    "                            model_save_location=model_save_location,\n",
    "                            use_gpu=use_gpu)\n",
    "model_picker.load_data('../data/events/'+str(event_id)+'.h5')\n",
    "\n",
    "model_picker.load_model(get_ensemble=False)\n",
    "\n",
    "# predictions = model_picker.predict()\n",
    "\n",
    "model_picker.save_output(str(event_id)+'_outputs.h5', convlstm_output, num_predictions=len(convlstm_output))\n",
    "# h5py2tif(h5_fname=str(event_id)+'_outputs.h5', \n",
    "#          meta_fname = '/vol_efthymios/NFS07/en279/SERVIR/TITO_test3/ML/nowcasting/data/events/1/metadata.json', \n",
    "#          tif_directory = '/vol_efthymios/NFS07/en279/SERVIR/TITO_test3/ML/nowcasting/data/events/1/predictions/',\n",
    "#          num_predictions=len(convlstm_output))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading config from configs/wa_imerg/PySTEPS.py ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading config from configs/wa_imerg/PySTEPS.py ...\n"
     ]
    }
   ],
   "source": [
    "from servir.utils.m_h5py2tif import h5py2tif\n",
    "from servir.core.model_picker import ModelPicker\n",
    "\n",
    "model_type = 'steps'\n",
    "model_config_location = 'configs/wa_imerg/PySTEPS.py'\n",
    "model_save_location = ''\n",
    "use_gpu = False\n",
    "produce_ensemble_outputs = False\n",
    "model_output_location = 'results/'\n",
    "steps_output = generate_outputs(data_loader, model_type, model_config_location, model_save_location, use_gpu, produce_ensemble_outputs=produce_ensemble_outputs, event_name = event_id,model_output_location = model_output_location)\n",
    "# tif file per prediction\n",
    "\n",
    "model_picker = ModelPicker(model_type=model_type, \n",
    "                            model_config_location=model_config_location, \n",
    "                            model_save_location=model_save_location,\n",
    "                            use_gpu=use_gpu)\n",
    "model_picker.load_data('../data/events/'+str(event_id)+'.h5')\n",
    "\n",
    "model_picker.load_model(get_ensemble=False)\n",
    "\n",
    "# predictions = model_picker.predict()\n",
    "\n",
    "model_picker.save_output(str(event_id)+'_outputs.h5', steps_output, num_predictions=len(steps_output))\n",
    "# h5py2tif(h5_fname=str(event_id)+'_outputs.h5', \n",
    "#          meta_fname = '/vol_efthymios/NFS07/en279/SERVIR/TITO_test3/ML/nowcasting/data/events/1/metadata.json', \n",
    "#          tif_directory = '/vol_efthymios/NFS07/en279/SERVIR/TITO_test3/ML/nowcasting/data/events/1/predictions/',\n",
    "#          num_predictions=len(steps_output),\n",
    "#          method='steps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lagrangian Persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading config from configs/wa_imerg/lagrangian_persistence.py ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading config from configs/wa_imerg/lagrangian_persistence.py ...\n",
      "(12, 900, 1295)\n"
     ]
    }
   ],
   "source": [
    "from servir.utils.m_h5py2tif import h5py2tif\n",
    "from servir.core.model_picker import ModelPicker\n",
    "\n",
    "model_type = 'lagrangian'\n",
    "model_config_location = 'configs/wa_imerg/lagrangian_persistence.py'\n",
    "model_save_location = ''\n",
    "use_gpu = False\n",
    "produce_ensemble_outputs = False\n",
    "model_output_location = 'results/'\n",
    "lagrangian_output = generate_outputs(data_loader, model_type, model_config_location, model_save_location, use_gpu, produce_ensemble_outputs=produce_ensemble_outputs, event_name = event_id,model_output_location = model_output_location)\n",
    "\n",
    "model_picker = ModelPicker(model_type=model_type, \n",
    "                            model_config_location=model_config_location, \n",
    "                            model_save_location=model_save_location,\n",
    "                            use_gpu=use_gpu)\n",
    "model_picker.load_data('../data/events/'+str(event_id)+'.h5')\n",
    "\n",
    "model_picker.load_model(get_ensemble=False)\n",
    "\n",
    "lagrangian_output = model_picker.predict()\n",
    "\n",
    "# lagrangian_output = lagrangian_output[None, :, :, :]\n",
    "model_picker.save_output(str(event_id)+'_outputs.h5', lagrangian_output, num_predictions=len(lagrangian_output))\n",
    "\n",
    "print(lagrangian_output.shape)\n",
    "# h5py2tif(h5_fname=str(event_id)+'_outputs.h5', \n",
    "#          meta_fname = '/vol_efthymios/NFS07/en279/SERVIR/TITO_test3/ML/nowcasting/data/events/1/metadata.json', \n",
    "#          tif_directory = '/vol_efthymios/NFS07/en279/SERVIR/TITO_test3/ML/nowcasting/data/events/1/predictions/',\n",
    "#          num_predictions=len(lagrangian_output))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading config from configs/wa_imerg/naive_persistence.py ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/volume/NFS/aa3328/miniconda3/envs/tito_env/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:285: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting predictions for batch 0\n",
      "The shape of the resulting array is:  (12, 360, 516)\n",
      "Advecting the radar rainfall fields took  0.008693218231201172  seconds\n",
      "The shape of the resulting array is:  (12, 360, 516)\n",
      "Advecting the radar rainfall fields took  0.002703428268432617  seconds\n",
      "The shape of the resulting array is:  (12, 360, 516)\n",
      "Advecting the radar rainfall fields took  0.0031270980834960938  seconds\n",
      "The shape of the resulting array is:  (12, 360, 516)\n",
      "Advecting the radar rainfall fields took  0.002671480178833008  seconds\n",
      "The shape of the resulting array is:  (12, 360, 516)\n",
      "Advecting the radar rainfall fields took  0.0027790069580078125  seconds\n",
      "The shape of the resulting array is:  (12, 360, 516)\n",
      "Advecting the radar rainfall fields took  0.0027272701263427734  seconds\n",
      "The shape of the resulting array is:  (12, 360, 516)\n",
      "Advecting the radar rainfall fields took  0.003042459487915039  seconds\n",
      "loading config from configs/wa_imerg/naive_persistence.py ...\n"
     ]
    }
   ],
   "source": [
    "from servir.utils.m_h5py2tif import h5py2tif\n",
    "from servir.core.model_picker import ModelPicker\n",
    "\n",
    "model_type = 'naive'\n",
    "model_config_location = 'configs/wa_imerg/naive_persistence.py'\n",
    "model_save_location = ''\n",
    "use_gpu = False\n",
    "produce_ensemble_outputs = False\n",
    "model_output_location = 'results/'\n",
    "naive_output = generate_outputs(data_loader, model_type, model_config_location, model_save_location, use_gpu, produce_ensemble_outputs=produce_ensemble_outputs, event_name = event_id,model_output_location = model_output_location)\n",
    "\n",
    "model_picker = ModelPicker(model_type=model_type, \n",
    "                            model_config_location=model_config_location, \n",
    "                            model_save_location=model_save_location,\n",
    "                            use_gpu=use_gpu)\n",
    "model_picker.load_data('../data/events/'+str(event_id)+'.h5')\n",
    "\n",
    "model_picker.load_model(get_ensemble=False)\n",
    "\n",
    "model_picker.save_output(str(event_id)+'_outputs.h5', naive_output, num_predictions=len(naive_output))\n",
    "\n",
    "# h5py2tif(h5_fname=str(event_id)+'_outputs.h5', \n",
    "#          meta_fname = '/vol_efthymios/NFS07/en279/SERVIR/TITO_test3/ML/nowcasting/data/events/1/metadata.json', \n",
    "#          tif_directory = '/vol_efthymios/NFS07/en279/SERVIR/TITO_test3/ML/nowcasting/data/events/1/predictions/',\n",
    "#          num_predictions=len(naive_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LINDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'linda'\n",
    "model_config_location = 'configs/wa_imerg/LINDA.py'\n",
    "model_save_location = ''\n",
    "use_gpu = False\n",
    "produce_ensemble_outputs = False\n",
    "model_output_location = 'results/'\n",
    "naive_output = generate_outputs(data_loader, model_type, model_config_location, model_save_location, use_gpu, produce_ensemble_outputs=produce_ensemble_outputs, event_name = event_id,model_output_location = model_output_location)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tito_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
